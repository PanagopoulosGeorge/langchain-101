import json
from typing import Dict, List, Optional, Union, Any
from pathlib import Path
import re

from langchain.prompts.chat import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)


class PromptLoader:
    """
    Loads prompts from structured JSON files generated by the doc_parser module.
    """
    
    def __init__(self, json_path: Union[str, Path]):
        """
        Initialize the PromptLoader with a path to a JSON file.
        
        Args:
            json_path: Path to the JSON file containing parsed prompts
        """
        self.json_path = Path(json_path) if isinstance(json_path, str) else json_path
        self.prompts = self._load_prompts()
        self.prompt_map = self._create_prompt_map()
        
    def _load_prompts(self) -> List[Dict]:
        """
        Load prompts from the JSON file.
        
        Returns:
            List of prompt dictionaries
        """
        if not self.json_path.exists():
            raise FileNotFoundError(f"Prompt file not found: {self.json_path}")
            
        with open(self.json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _create_prompt_map(self) -> Dict[str, Dict]:
        """
        Create a mapping of prompt titles to prompt data for quick lookup.
        
        Returns:
            Dictionary mapping prompt titles to their data
        """
        return {prompt['title']: prompt for prompt in self.prompts}
    
    def get_prompt_by_title(self, title: str) -> Optional[Dict]:
        """
        Retrieve a prompt by its title.
        
        Args:
            title: The title of the prompt to retrieve
            
        Returns:
            The prompt dictionary or None if not found
        """
        return self.prompt_map.get(title, "Not fount")
    
    def get_prompts_by_type(self, prompt_type: str) -> List[Dict]:
        """
        Retrieve all prompts of a specific type.
        
        Args:
            prompt_type: The type of prompts to retrieve
            
        Returns:
            List of prompt dictionaries matching the specified type
        """
        return [prompt for prompt in self.prompts if prompt['prompt_type'] == prompt_type]
    
    def get_prompts_in_sequence(self) -> List[Dict]:
        """
        Retrieve all prompts ordered by their sequence_id.
        
        Returns:
            List of prompt dictionaries in sequence order
        """
        return sorted(self.prompts, key=lambda x: x['sequence_id'])
    
    def serialize_prompts(self, prompt_type: str) -> List[Any]:
        """
        Serialize prompts of a specific type into LangChain-compatible components.

        Args:
            prompt_type: The type of prompts to serialize

        Returns:
            List of LangChain prompt templates
        """
        prompts = self.get_prompts_by_type(prompt_type)
        serialized_prompts = []

        for prompt in prompts:
            if prompt_type in ["BACKGROUND-KNOWLEDGE", "DOMAIN"]:
                serialized_prompts.append(SystemMessagePromptTemplate.from_template(prompt['query']))
            elif prompt_type == "EXAMPLE":
                # For examples, we create a pair of messages (human question and AI answer)
                # First, parse the example to extract question and answer parts
                parts = self._parse_example_prompt(prompt['query'])
                if parts.get('question') and parts.get('answer'):
                    serialized_prompts.append(HumanMessagePromptTemplate.from_template(parts['question']))
                    serialized_prompts.append(AIMessagePromptTemplate.from_template(parts['answer']))
                else:
                    # If parsing fails, use a placeholder
                    serialized_prompts.append(MessagesPlaceholder(variable_name=prompt['title']))
            elif prompt_type == "USER":
                serialized_prompts.append(HumanMessagePromptTemplate.from_template(prompt['query'] + "\n\nFormat the final rtec rule in triple backticks like this:\n```prolog\n```"))

        return serialized_prompts
    
    def _parse_example_prompt(self, example_text: str) -> Dict[str, str]:
        """
        Parse an example prompt to extract question and answer parts.
        
        Args:
            example_text: The text of the example prompt
            
        Returns:
            Dictionary with 'question' and 'answer' keys
        """
        result = {}
        
        # Try to find patterns like "Example X: ... Question: ... Answer: ..."
        if "example" in example_text.lower() and "answer" in example_text.lower():
            question_start = re.search(r"[\s\d\W]{0,3}Example[\s\d:]{0,3}", example_text).end()
            answer_start_idx = re.search(r"[\s\d\W]{0,3}Answer[\s\d:]{0,3}", example_text).start()
            answer_start = re.search(r"[\s\d\W]{0,3}Answer[\s\d:]{0,3}", example_text).end()
            
            if question_start >= 0 and answer_start > question_start:
                result['question'] = example_text[question_start:answer_start_idx].strip()
                result['answer'] = example_text[answer_start:].strip()

        return result
    
    def create_chat_prompt_template(self, 
                                   train_prompt_types: List[str] = ["BACKGROUND-KNOWLEDGE", "DOMAIN", "EXAMPLE"],
                                   user_prompt: str = None) -> ChatPromptTemplate:
        """
        Create a ChatPromptTemplate combining different types of prompts.
        
        Args:
            system_prompts: Titles of system prompts to include (BACKGROUND-KNOWLEDGE or DOMAIN)
            example_prompts: Titles of example prompts to include
            user_prompt: Title of the user prompt to include
            input_variables: List of input variables for the template
            
        Returns:
            A ChatPromptTemplate instance
        """
        messages = []
        
        # Add system prompts
        if train_prompt_types:
            for type in train_prompt_types:
                serialized_prompts = self.serialize_prompts(type)
                messages = [*messages, *serialized_prompts]
        
        # Add user prompt or a generic human message template
        if user_prompt:
            prompt = self.get_prompt_by_title(user_prompt)
            if prompt:
                messages.append(HumanMessagePromptTemplate.from_template(prompt['query']  + "\n\nFormat the final RTEC rule in triple backticks like this:\n```prolog\n```"))
                return ChatPromptTemplate.from_messages(messages=messages)
        print("No input prompt was found.")
        print("No user prompt was found within the parsed document.")
        input_prompt = input("Please provide your input request: ")
        messages.append(HumanMessagePromptTemplate.from_template(input_prompt))
        
        return ChatPromptTemplate.from_messages(messages=messages)